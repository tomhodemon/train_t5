name: "default"
data: 
  corruption_rate: 0.15 # from paper
  mean_corrupted_span_length: 3.0 # from paper
  input_length: 141 # input length before corruption (has been found empirically)
  target_length: 29 # target length generated from corrupted input (deduced from input_length)
  save_path: "data/processed"
tokenizer:
  name: "t5-small"
model:
  name: "t5-small"
optimizer:
  base_lr: 1
train:
  batch_size: 32
  max_seq_len: 128
  max_steps: 20_000
  logging_steps: 50
  eval_steps: 300
  epochs: 3