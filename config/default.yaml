name: "default"
data: 
  corruption_rate: 0.15 # from paper
  mean_corrupted_span_length: 3.0 # from paper
  input_length: 141 # input length before corruption (has been found empirically)
  target_length: 29 # target length generated from corrupted input (deduced from input_length)
tokenizer:
  name: "t5-small"
model:
  name: "t5-small"
  drop_decoder_ffn: false
  drop_encoder_ffn: false
  share_decoder_ffn: false
  share_encoder_ffn: false
optimizer:
  # The paper does not mention any particular learning rate value. It only states:
  # "During pre-training, we use an “inverse square root” learning rate schedule... 
  # This sets a constant learning rate of 0.01 for the first 10e4 steps, 
  # then exponentially  decays the learning rate until pre-training is over."
  base_lr: 1
train:
  seed: 45
  batch_size: 128
  max_seq_len: 512
  max_steps: 1000
  logging_steps: 20
  eval_steps: 50
  epochs: 1
